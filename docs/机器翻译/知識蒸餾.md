https://zhuanlan.zhihu.com/p/530294872



## TinyBERT
### 研究背景
- 语言模型预训练，例如 BERT，显着提高了许多自然语言处理任务的性能。
- 然而，预训练的语言模型通常计算量很大，因此很难在资源受限的设备上有效地执行它们。
- 最近的研究（Kovaleva 等人，2019；Michel 等人，2019；Voita 等人，2019）表明 PLM 存在冗余。因此，在保持 PLM 性能的同时减少计算开销和模型存储是至关重要且可行的。已经提出了许多模型压缩技术 (Han et al., 2016) 来加速深度模型推理并在保持准确性的同时减小模型大小。最常用的技术包括量化 (Gong et al., 2014)、权重修剪 (Han et al., 2015) 和知识蒸馏 (KD) (Romero et al., 2014)。

### 主要工作
![](./image/The%20illustration%20of%20TinyBERT%20learning.jpg)
- 为了在保持准确性的同时加速推理并减小模型大小，我们首先提出了一种新的 Transformer 蒸馏方法，设计了三种类型的损失函数来适应来自 BERT 层的不同表示：1）嵌入层的输出； 2）从Transformer层导出的隐藏状态和注意力矩阵； 3）预测层输出的logits。基于注意力的拟合受到最近发现（Clark 等人，2019 年）的启发，即 BERT 学习的注意力权重可以捕获大量的语言知识，因此它鼓励语言知识可以很好地从教师 BERT 转移到学生 TinyBERT。通过利用这种新的 KD 方法，大型“教师”BERT 中编码的大量知识可以有效地转移到小型“学生”TinyBERT 中。该方法专为基于 Transformer 的模型的知识蒸馏 (KD) 而设计。
- 然后，为 TinyBERT 引入了一个新的两阶段学习框架，该框架在预训练和特定任务的学习阶段都执行 Transformer 蒸馏。该框架确保 TinyBERT 可以捕获 BERT 中的通用领域以及特定于任务的知识。

### 核心工作
TinyBERT Learning
- 提出了一种新颖的两阶段学习框架，包括通用蒸馏和特定任务蒸馏，如图 1 所示。通用蒸馏帮助 TinyBERT 学习嵌入在预训练 BERT 中的丰富知识，这对改进TinyBERT 的泛化能力。特定于任务的蒸馏进一步向 TinyBERT 传授来自微调 BERT 的知识。通过两步蒸馏，我们可以大大缩小教师和学生模型之间的差距。

###  结论
具有 4 层的 TinyBERT41 在经验上是有效的，在 GLUE 基准测试中其教师 BERTBASE 的性能达到了 96.8% 以上，同时在推理上缩小了 7.5 倍，速度提高了 9.4 倍。 TinyBERT4 在 BERT 蒸馏上也明显优于 4 层最先进的基线，只有 28% 的参数和 31% 的推理时间。此外，具有 6 层的 TinyBERT6 的性能与其教师 BERTBASE 不相上下。